{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Importing library\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "from scipy.stats.stats import pearsonr\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Importing dataset\n",
    "df = pd.read_csv(\"dataset/train_processed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the data so that all values are between 0 and 1. So we first remove 'Date' which is the only non numerical attribute. The we apply range normalisation and at the end we add 'Date'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_date=df['Date']\n",
    "df_no_date=df.drop(['Date'], axis=1)\n",
    "df_norm= (df_no_date - df_no_date.min()) / (df_no_date.max() - df_no_date.min())\n",
    "df=df_norm\n",
    "df['Date']=df_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the dataset in 4 parts:\n",
    "* df_train: the training set (all the rows before 2017-09-01). We use it to train our model.\n",
    "* df_val1: the first validation set (all the rows between 2017-09-01 and 2017-11-01). We use it to tune the hyperparameter of our model.\n",
    "* df_val2: the second validation set (all the rows between 2017-11-01 and 2018-01-01). We use it to select the best model.\n",
    "* df_val3: the third validation set (all the rows after 2018-01-01). We use it to check that our final model works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_val3=df.loc[(df['Date']>='2018-01-01')]\n",
    "df_val3_rest=df.loc[(df['Date']<'2018-01-01')]\n",
    "df_val2=df_val3_rest.loc[(df_val3_rest['Date']>='2017-11-01')]\n",
    "df_val2_rest=df.loc[(df['Date']<'2017-11-01')]\n",
    "df_val1=df_val2_rest.loc[(df_val2_rest['Date']>='2017-09-01')]\n",
    "df_train=df.loc[(df['Date']<'2017-09-01')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We remove 'Date' from our set because it is only used as index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_train_clear=df_train.drop(['Date'], axis=1)\n",
    "df_val1_clear=df_val1.drop(['Date'], axis=1)\n",
    "df_val2_clear=df_val2.drop(['Date'], axis=1)\n",
    "df_val3_clear=df_val3.drop(['Date'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the target valuesy for all the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y=df_train_clear['NumberOfSales']\n",
    "y_val1=df_val1_clear['NumberOfSales']\n",
    "y_val2=df_val2_clear['NumberOfSales']\n",
    "y_val3=df_val3_clear['NumberOfSales']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the training input samples for all the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X=df_train_clear.drop(['StoreID','NumberOfSales','NumberOfCustomers'], axis=1)\n",
    "X_val1=df_val1_clear.drop(['StoreID','NumberOfSales','NumberOfCustomers'], axis=1)\n",
    "X_val2=df_val2_clear.drop(['StoreID','NumberOfSales','NumberOfCustomers'], axis=1)\n",
    "X_val3=df_val3_clear.drop(['StoreID','NumberOfSales','NumberOfCustomers'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the dataframe in which we put the result of the hyperparameter tuning. It is a table with the following colums:\n",
    "* num_of_trees: the number of trees of the random forest\n",
    "* depth: the maximum depth of the random forest\n",
    "* num_of_attr: the maximum number of attributes considered at each split\n",
    "* mse: the mean squared error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "columns=['num_of_trees','depth','num_of_attr','mse','mae']\n",
    "index=range(1,1125000)\n",
    "hp=pd.DataFrame(index=index,columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cycle over all the combinations of the hyperparameters. At each iterations we do the following operations:\n",
    "* computer the model using the training set\n",
    "* predict the samples in the first validation set using the model\n",
    "* compute the mean squared error\n",
    "* add the result in the dataframe for the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nt=201\n",
    "na=12\n",
    "depth=21\n",
    "\n",
    "feature_names = list(X.columns)\n",
    "\n",
    "forest = RandomForestRegressor(max_depth=depth, random_state=0, n_estimators=nt, max_features=na, n_jobs=-1)\n",
    "forest.fit(X, y)\n",
    "y_pred=forest.predict(X_val1)\n",
    "mse_val1=mean_squared_error(y_val1,y_pred)\n",
    "mae_val1=mean_absolute_error(y_val1,y_pred)\n",
    "print(\"con tutte le feature\", sqrt(mse_val1), mae_val1)\n",
    "\n",
    "for feature in feature_names[21:]:\n",
    "    X=df_train_clear.drop(['StoreID','NumberOfSales','NumberOfCustomers'], axis=1)\n",
    "    X_val1=df_val1_clear.drop(['StoreID','NumberOfSales','NumberOfCustomers'], axis=1)\n",
    "    X=X.drop([feature], axis=1)\n",
    "    X_val1=X_val1.drop([feature], axis=1)\n",
    "    forest = RandomForestRegressor(max_depth=depth, random_state=0, n_estimators=nt, max_features=na, n_jobs=-1)\n",
    "    forest.fit(X, y)\n",
    "    y_pred=forest.predict(X_val1)\n",
    "    mse_val1=mean_squared_error(y_val1,y_pred)\n",
    "    mae_val1=mean_absolute_error(y_val1,y_pred)\n",
    "    print(feature, sqrt(mse_val1), mae_val1)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
