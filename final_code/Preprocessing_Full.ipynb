{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import norm\n",
    "from scipy.stats.stats import pearsonr\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "# Option to display all the dataframe columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# Importing dataset\n",
    "TRAIN_PATH = \"dataset/train_original.csv\"\n",
    "TEST_PATH = \"dataset/test.csv\"\n",
    "\n",
    "df_train = pd.read_csv(TRAIN_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "train_instances = df_train.shape[0]\n",
    "test_instances = df_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate train and test to perform preprocessing on both\n",
    "\n",
    "df = pd.concat([df_train, df_test], sort=False).reset_index()\n",
    "\n",
    "# Transform dates from '%d/%m/%Y' to datetime objects.\n",
    "\n",
    "def transform_date(x):\n",
    "    date = datetime.datetime.strptime(x, '%d/%m/%Y')\n",
    "    return date\n",
    "\n",
    "df['Date'] = df['Date'].map(transform_date)\n",
    "\n",
    "# Transform categorical features\n",
    "\n",
    "df[\"StoreType\"] = df[\"StoreType\"].map({'Standard Market': 0, 'Super Market': 1, 'Hyper Market': 2, 'Shopping Center': 3})\n",
    "df = pd.get_dummies(df, columns=['AssortmentType'])\n",
    "\n",
    "for e in ['Rain', 'Snow', 'Fog', 'Hail', 'Thunderstorm']:\n",
    "    df[e] = df.loc[:, 'Events'].astype('str').apply(lambda x: int(e in x))\n",
    "    \n",
    "df = df.drop(labels=['NumberOfCustomers', 'Region', 'AssortmentType_General', 'Events'], axis=1)\n",
    "    \n",
    "# Add date-related features\n",
    "\n",
    "def is_saturday(day):\n",
    "    return int(day.weekday() == 5)\n",
    "\n",
    "def is_sunday(day):\n",
    "    return int(day.weekday() == 6)\n",
    "\n",
    "df['IsSaturday'] = df['Date'].map(is_saturday)\n",
    "df['IsSunday'] = df['Date'].map(is_sunday)    \n",
    "\n",
    "def ordinal_date(x):\n",
    "    d = x['Date']\n",
    "    date = datetime.date(d.year, d.month, d.day)\n",
    "    return datetime.date.toordinal(date)\n",
    "\n",
    "df['OrdinalDate'] = df.loc[:, ['Date']].apply(ordinal_date, axis=1)\n",
    "\n",
    "base_date = min(df['OrdinalDate'])\n",
    "\n",
    "df['OrdinalDate'] = df['OrdinalDate'].apply(lambda x: x - base_date)\n",
    "\n",
    "# Divide dataset in chunks based on StoreID\n",
    "\n",
    "stores = []\n",
    "for store in df['StoreID'].unique():\n",
    "    stores.append( df.loc[(df['StoreID'] == store)] )\n",
    "    \n",
    "def was_open_yesterday(x):\n",
    "    date = x['OrdinalDate']\n",
    "    store_index = x['StoreID'] - 1000\n",
    "    if date < 30:\n",
    "        return 1\n",
    "    that_date = stores[store_index].loc[(stores[store_index]['OrdinalDate'] == date - 1)]\n",
    "    if len(that_date) < 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return that_date['IsOpen'].item()\n",
    "    \n",
    "df['WasOpenYesterday'] = df.loc[:, ['OrdinalDate', 'StoreID']].apply(was_open_yesterday, axis=1)\n",
    "\n",
    "def is_open_tomorrow(x):\n",
    "    date = x['OrdinalDate']\n",
    "    store_index = x['StoreID'] - 1000\n",
    "    if date == 729:\n",
    "        return 1\n",
    "    that_date = stores[store_index].loc[(stores[store_index]['OrdinalDate'] == date + 1)]\n",
    "    if len(that_date) < 1:\n",
    "        return 0\n",
    "    else:\n",
    "        return that_date['IsOpen'].item()\n",
    "\n",
    "df['IsOpenTomorrow'] = df.loc[:, ['OrdinalDate', 'StoreID']].apply(is_open_tomorrow, axis=1)\n",
    "\n",
    "# Drop rows where the store is closed (IsOpen = 0)\n",
    "\n",
    "df = df.drop(df[df.IsOpen == 0].index)\n",
    "df = df.drop(labels=['IsOpen'], axis=1)\n",
    "\n",
    "# Add feature for the mean sales for that month\n",
    "\n",
    "df['Month'] = df.Date.map(lambda d: d.strftime('%m'))\n",
    "mean_month_sales = df.groupby('Month').mean().NumberOfSales.to_dict()\n",
    "df['MeanMonthSales'] = df.Date.map(lambda d: mean_month_sales[d.strftime('%m')])\n",
    "df = df.drop(labels=['Month'], axis=1)\n",
    "\n",
    "# Add feature for the mean sales for that store\n",
    "\n",
    "mean_store_sales = dict()\n",
    "\n",
    "for store_id in df.StoreID.unique():\n",
    "    rows = df[df.StoreID == store_id]\n",
    "    mean_store_sales[store_id] = rows.NumberOfSales.mean()\n",
    "    \n",
    "df['MeanStoreSales'] = df.StoreID.map(mean_store_sales)\n",
    "\n",
    "# Add feature for the std of sales for that store\n",
    "\n",
    "std_store_sales = dict()\n",
    "\n",
    "for store_id in df.StoreID.unique():\n",
    "    rows = df[df.StoreID == store_id]\n",
    "    std_store_sales[store_id] = rows.NumberOfSales.std()\n",
    "    \n",
    "df['StdStoreSales'] = df.StoreID.map(std_store_sales)\n",
    "\n",
    "# Impute missing values with RandomForestRegressor\n",
    "\n",
    "def impute_missing(df, feature):\n",
    "    # Rearrange columns and pick features subset for prediction\n",
    "    cols = df.columns.tolist()\n",
    "    cols.insert(0, cols.pop(cols.index(feature)))\n",
    "    cols.pop(cols.index('Date'))\n",
    "    cols.pop(cols.index('StoreID'))\n",
    "    \n",
    "    na_cols = df.columns[df.isna().any()].tolist()\n",
    "    na_cols.pop(na_cols.index(feature))\n",
    "    \n",
    "    cols = [item for item in cols if item not in na_cols]\n",
    "    \n",
    "    feature_df = df[cols]\n",
    "    \n",
    "    # Split into sets with known and unknown feature values\n",
    "    known = feature_df.loc[ (df[feature].notnull()) ]\n",
    "    unknown = feature_df.loc[ (df[feature].isnull()) ]\n",
    "    \n",
    "    y = known.values[:, 0]\n",
    "    X = known.values[:, 1::]\n",
    "    \n",
    "    # Create and fit a model\n",
    "    rtr = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "    rtr.fit(X, y)\n",
    "    \n",
    "    # Predict missing values\n",
    "    predicted = rtr.predict(unknown.values[:, 1::])\n",
    "    \n",
    "    # Assign those predictions to the full data set\n",
    "    df.loc[ (df[feature].isnull()), feature ] = predicted\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = impute_missing(df, 'Min_VisibilitykM')\n",
    "df = impute_missing(df, 'Mean_VisibilityKm')\n",
    "df = impute_missing(df, 'Max_VisibilityKm')\n",
    "df = impute_missing(df, 'CloudCover')\n",
    "df = impute_missing(df, 'Max_Gust_SpeedKm_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train         = df.loc[(df['Date'] <  '2018-03-01')]\n",
    "df_test          = df.loc[(df['Date'] >= '2018-03-01')]\n",
    "df_train_no_val2 = df_train.loc[(df_train['Date'] <  '2018-01-01')]\n",
    "df_val2          = df_train.loc[(df_train['Date'] >= '2018-01-01')]\n",
    "\n",
    "# Outlier elimination\n",
    "\n",
    "def remove_outliers(df):\n",
    "    df = df.drop(df[(df.NumberOfSales > df.MeanStoreSales+4.5*df.StdStoreSales) |\n",
    "                    (df.NumberOfSales < df.MeanStoreSales-3*df.StdStoreSales)].index)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_train         = remove_outliers(df_train)\n",
    "df_train_no_val2 = remove_outliers(df_train_no_val2)\n",
    "\n",
    "msk              = np.random.rand(len(df_train_no_val2)) < 0.9\n",
    "df_val1_rand     = df_train_no_val2[~msk]\n",
    "df_train_rand    = df_train_no_val2[msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(\"processed/train.csv\", index=False)\n",
    "df_test.to_csv(\"processed/test.csv\", index=False)\n",
    "df_train_no_val2.to_csv(\"processed/train_no_val2.csv\", index=False)\n",
    "df_val2.to_csv(\"processed/val2.csv\", index=False)\n",
    "df_val1_rand.to_csv(\"processed/val1_rand.csv\", index=False)\n",
    "df_train_rand.to_csv(\"processed/train_rand.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
