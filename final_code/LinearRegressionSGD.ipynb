{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from sklearn.linear_model.stochastic_gradient import SGDRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing dataset\n",
    "train_no_val2 = pd.read_csv(\"../dataset/train_no_val2.csv\", index_col=0)\n",
    "train_no_val1 = train_no_val2.loc[(train_no_val2['Date']<'2017-11-01')]\n",
    "\n",
    "val1 = train_no_val2.loc[((train_no_val2['Date']>='2017-11-01') & (train_no_val2['Date']<='2017-12-31'))]\n",
    "val2 = pd.read_csv(\"../dataset/val2.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We save 'Date', 'StoreID' and 'Region' for the final csv with all the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_no_val1.OrdinalDate = train_no_val1.OrdinalDate.map(lambda x: x % 365)\n",
    "train_no_val2.OrdinalDate = train_no_val2.OrdinalDate.map(lambda x: x % 365)\n",
    "\n",
    "val1.OrdinalDate = val1.OrdinalDate.map(lambda x: x % 365)\n",
    "val2.OrdinalDate = val2.OrdinalDate.map(lambda x: x % 365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_no_val1 = train_no_val1['NumberOfSales']\n",
    "y_train_no_val2 = train_no_val2['NumberOfSales']\n",
    "\n",
    "y_val1 = val1['NumberOfSales']\n",
    "y_val2 = val2['NumberOfSales']\n",
    "\n",
    "X_train_no_val1 = train_no_val1.drop(['StoreID','Date', 'NumberOfSales', 'Region'], axis=1)\n",
    "X_train_no_val2 = train_no_val2.drop(['StoreID','Date', 'NumberOfSales', 'Region'], axis=1)\n",
    "\n",
    "X_val1 = val1.drop(['StoreID','Date', 'NumberOfSales', 'Region'], axis=1)\n",
    "X_val2 = val2.drop(['StoreID','Date', 'NumberOfSales', 'Region'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the values and set the 'OrdinalDate' on the day of of the year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_val1 = (X_train_no_val1 - X_train_no_val1.min()) / (X_train_no_val1.max() - X_train_no_val1.min())\n",
    "X_train_no_val2 = (X_train_no_val2 - X_train_no_val2.min()) / (X_train_no_val2.max() - X_train_no_val2.min())\n",
    "\n",
    "X_val1 = (X_val1 - X_val1.min()) / (X_val1.max() - X_val1.min())\n",
    "X_val2 = (X_val2 - X_val2.min()) / (X_val2.max() - X_val2.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_no_val1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to find the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_params = {'loss_function' : [], 'alpha' : [], 'epsilon' : [], 'learning_rate' : [], 'error' : [],'mse' : [], 'mae' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def errore(ypred, y, val):\n",
    "    val = val.reset_index(drop=True)\n",
    "    y = y.reset_index(drop=True)\n",
    "\n",
    "    columns=['Date', 'StoreID', 'RegionID', 'SalesPredicted', 'SalesReal']\n",
    "    index=range(y.shape[0])\n",
    "    result=pd.DataFrame(index=index,columns=columns)\n",
    "\n",
    "    result['Date']=val['Date']\n",
    "    result['StoreID']=val['StoreID']\n",
    "    result['RegionID']=val['Region']\n",
    "    result['SalesPredicted']=ypred\n",
    "    result['SalesReal']=y\n",
    "    \n",
    "\n",
    "    # Transform dates from '%Y-%m-%d' to datetime objects.\n",
    "    def transform_date(x):\n",
    "        date = datetime.datetime.strptime(x, '%Y-%m-%d')\n",
    "        return date\n",
    "\n",
    "    result['Date'] = result['Date'].map(transform_date)\n",
    "    result['Month'] = result.Date.map(lambda d: d.strftime('%Y-%m'))\n",
    "    result = result.groupby(['StoreID', 'RegionID', 'Month']).sum().reset_index()[['Month', 'StoreID', 'RegionID', 'SalesPredicted', 'SalesReal']]\n",
    "\n",
    "    result['SalesError'] = abs(result.SalesPredicted - result.SalesReal)\n",
    "\n",
    "    region_error = (result.groupby('RegionID').sum().SalesError / result.groupby('RegionID').sum().SalesReal).reset_index().rename(columns={0: 'RegionError'}).set_index('RegionID')\n",
    "    total_error = np.mean(region_error.RegionError)\n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_fun = ['squared_loss', 'epsilon_insensitive']\n",
    "learning_r = ['constant', 'optimal', 'invscaling']\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for l_f in loss_fun:\n",
    "    for l_r in learning_r:\n",
    "        for a in np.linspace(.000001, .00001 , 100):\n",
    "            for e in np.linspace(.1, .01, 5):\n",
    "                sgd = SGDRegressor(loss=l_f, alpha=a, epsilon=e, learning_rate=l_r, shuffle=True, random_state=1234567890)\n",
    "                dict_params['loss_function'].append(l_f)\n",
    "                dict_params['alpha'].append(a)\n",
    "                dict_params['epsilon'].append(e)\n",
    "                dict_params['learning_rate'].append(l_r)\n",
    "                sgd.fit(X_train_no_val1, y_train_no_val1)\n",
    "                y_pred = sgd.predict(X_val1)\n",
    "                mse = sqrt(mean_squared_error(y_val1, y_pred))\n",
    "                mae = mean_absolute_error(y_val1, y_pred)\n",
    "                err = errore(y_pred, y_val1, val1)\n",
    "                dict_params['mse'].append(mse)\n",
    "                dict_params['mae'].append(mae)\n",
    "                dict_params['error'].append(err)\n",
    "                \n",
    "                print(l_f, l_r, a, e, mse, mae, err)\n",
    "            \n",
    "end = time.time() - start\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.DataFrame(dict_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extrapolate the best parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_err = params.error.min()\n",
    "best_params = params[(params['error'] == min_err)].iloc[0]\n",
    "\n",
    "print(\"Minimum error: %f\" %min_err)\n",
    "\n",
    "loss_f = best_params['loss_function']\n",
    "alp = best_params['alpha']\n",
    "eps = best_params['epsilon']\n",
    "learn_rate = best_params['learning_rate']\n",
    "\n",
    "print(\"Loss function: \" + loss_f + \"; alpha: %f: epsilon: %f\" %(alp, eps) + \"; learning rate: \" + learn_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate our model on the last two months of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDRegressor(loss=loss_f, alpha=alp, epsilon=eps, learning_rate=learn_rate, shuffle=True, random_state=1234567890)\n",
    "start = time.time()\n",
    "sgd.fit(X_train_no_val2, y_train_no_val2)\n",
    "y_pred2 = sgd.predict(X_val2)\n",
    "end = time.time() - start\n",
    "mse2 = sqrt(mean_squared_error(y_val2, y_pred2))\n",
    "mae2 = mean_absolute_error(y_val2, y_pred2)\n",
    "err = errore(y_pred2, y_val2, val2)\n",
    "\n",
    "print(\"mse: %f, mae: %f, error: %f, time: %f\" %(mse2, mae2, err, end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_val2 = {'Date' : date_val2, 'StoreID' : store_val2, 'RegionID' : region_val2, 'SalesPredicted': [], 'SalesReal': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_val2['SalesReal'] = list(y_val2.to_frame()['NumberOfSales'].values)\n",
    "dict_val2['SalesPredicted'] = list(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(dict_val2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.to_csv(\"../results/result_linear_regression.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
